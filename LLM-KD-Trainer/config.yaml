### Model
student_path: "Qwen2.5-1.5B-Instruct"
teacher_path: "Qwen2.5-72B-Instruct"


# LoRA Configuration
lora_r: 8
lora_alpha: 256
lora_target: "q_proj,v_proj,k_proj,o_proj,gate_proj,up_proj,down_proj"
lora_dropout: 0.1
lora_task_type: "CAUSAL_LM"

### Data
data_path: "data.json"
max_sequence_length: 512

### Training
output_dir: "./results"
save_model_dir: "./saves"
num_train_epochs: 10
per_device_train_batch_size: 2
gradient_accumulation_steps: 16
learning_rate: 0.0005
lr_scheduler_type: "cosine"
warmup_steps: 0

### Trainer
use_entropy_loss: true
logging_steps: 10
report_to: "tensorboard"
save_strategy: "epoch"
save_total_limit: 10
bf16: true
dataloader_num_workers: 8
dataloader_pin_memory: true

### Distillation
temperature: 2.0
padding_id: -100
