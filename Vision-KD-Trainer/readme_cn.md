# 基于 KL 散度的知识蒸馏

本仓库实现了基于 KL 散度的知识蒸馏技术，该技术通过训练一个轻量级的 “学生” 模型，使其模仿更复杂的大型 “教师” 模型的行为。核心思想是通过最小化两个模型输出分布之间的 KL 散度，将教师模型的知识迁移到学生模型中，同时结合基于真实标签的标准分类损失。

## 概述

知识蒸馏是一种模型压缩方法，它利用高性能教师模型所捕获的 “知识” 来训练更小的学生模型。本实现重点通过 KL 散度（一种衡量概率分布差异的指标）进行知识迁移，确保学生模型不仅能从真实标签中学习，还能模仿教师模型的输出模式，这通常比单独训练学生模型能获得更好的泛化能力。

## 核心功能



*   **数据集处理**：加载并预处理 CIFAR-10 数据集，用于模型的训练和评估。

*   **模型定义**：


    *   复杂的高容量教师模型

    *   为提高效率设计的轻量级学生模型

*   **训练函数**：


    *   基线训练（使用标准交叉熵损失），适用于教师模型和学生模型

    *   蒸馏训练，结合两种损失：


        *   KL 散度损失（使学生输出与教师输出对齐）

        *   交叉熵损失（使学生输出与真实标签对齐）

*   **评估**：用于比较教师模型、基线学生模型和蒸馏训练的学生模型性能（如准确率）的工具。

## 使用方法



1.  准备依赖环境（如 PyTorch、torchvision），用于处理 CIFAR-10 数据集和模型训练。

2.  运行训练脚本：

*   使用标准交叉熵损失训练教师模型

*   使用交叉熵损失训练基线学生模型（不使用蒸馏）

*   结合 KL 散度损失（针对教师输出）和交叉熵损失（针对真实标签），对学生模型进行知识蒸馏训练

1.  使用评估代码比较所有模型的性能指标（如测试准确率）。

展示了识蒸馏如何帮助轻量级学生模型逼近大型教师模型的性能。